{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d4dd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import optuna\n",
    "\n",
    "os.environ[\"OPENPI_DATA_HOME\"] = (\n",
    "    \"/mnt/virtual_ai0001071-01239_SR006-nfs2/apanasevich/openpi/assets\"\n",
    ")\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/virtual_ai0001071-01239_SR006-nfs2/.cache/huggingface\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/mnt/virtual_ai0001071-01239_SR006-nfs2/.cache\"\n",
    "\n",
    "from lerobot.common.datasets.torch_transforms import compose\n",
    "from lerobot.common.datasets.create_dataloader import create_lerobot_dataloader\n",
    "from lerobot.common.datasets.data_config import (\n",
    "    LeRobotAgibotTwoFingerDataConfig,\n",
    "    LeRobotAgibotDexHandDataConfig,\n",
    ")\n",
    "\n",
    "from beast.bspline_tokenizer import BSpline_Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "import hydra\n",
    "from rich_argparse import RichHelpFormatter\n",
    "from accelerate.utils import broadcast_object_list\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from lerobot.common.utils.inference_transforms import get_torch_input_transforms, get_torch_output_transforms\n",
    "from accelerate.utils import InitProcessGroupKwargs\n",
    "from datetime import timedelta\n",
    "\n",
    "from lerobot.common.datasets.data_config import (\n",
    "    AssetsConfig as LeRobotAssetsConfig\n",
    ")\n",
    "from lerobot.common.datasets.data_config import DataConfig as LeRobotBaseDataConfig\n",
    "from lerobot.common.policies.pi0.configuration_pi0 import PI0Config\n",
    "from lerobot.common.policies.pi0fast.configuration_pi0fast import PI0FASTConfig\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from lerobot.common.utils.normalize import RunningStats, save as save_stats\n",
    "import torch.distributed as dist\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from lerobot.common.datasets.create_dataloader import create_lerobot_dataset_by_config\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Any\n",
    "import random\n",
    "import logging\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def create_train_val_datasets_distributed(\n",
    "    data_config_factory: Any,\n",
    "    model_config: Any,\n",
    "    assets_dirs: str,\n",
    "    accelerator: Accelerator,  # Add accelerator parameter\n",
    "    val_split: float = 0.1,\n",
    "    seed: int = 42,\n",
    "    map_to_unified_space: bool = False,\n",
    "    use_validation_list: bool = False,\n",
    "    recompute_norm_stats: bool = False,\n",
    "    **dataset_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Create train and validation datasets with distributed-aware episode splitting.\n",
    "    Only main process loads metadata, then broadcasts splits to all processes.\n",
    "    \n",
    "    Logic flow:\n",
    "    1. First, check if validation episodes are provided in data config via 'validation_episodes' key\n",
    "    2. If validation episodes files are found, load them and create val_episodes_dict from them\n",
    "    3. If no validation episodes files are found, do random shuffling and splitting based on val_split ratio\n",
    "    4. Train episodes are always created as the complement of validation episodes\n",
    "    \n",
    "    The validation_episodes JSON file can contain:\n",
    "    - Direct list: [0, 1, 2, ...] (applies to all datasets)\n",
    "    - Multi-dataset dict: {dataset_name: [0, 1, 2, ...]}\n",
    "    \"\"\"\n",
    "    class DummyFactory:\n",
    "        def __init__(self, cfg_item):\n",
    "            self.cfg_item = cfg_item\n",
    "        \n",
    "        def create(self, *args, **kwargs):\n",
    "            return self.cfg_item\n",
    "    \n",
    "    # Get data config to determine dataset type (needed for output pipeline creation)\n",
    "    data_cfg = data_config_factory.create(assets_dirs, model_config)\n",
    "    \n",
    "    # Check if we have mixture configs\n",
    "    if hasattr(data_cfg, 'mixture_configs') and data_cfg.mixture_configs:\n",
    "        # Multi-dataset scenario\n",
    "        dataset_configs = data_cfg.mixture_configs\n",
    "        dataset_names = [cfg_item.repo_id for cfg_item in dataset_configs] \n",
    "        for i, cfg_item in enumerate(dataset_configs):\n",
    "            cfg_name = dataset_names[i]\n",
    "    else:\n",
    "        # Single dataset scenario\n",
    "        dataset_configs = [data_cfg]\n",
    "        dataset_names = [\"main\"]\n",
    "    \n",
    "    # ONLY MAIN PROCESS loads metadata and creates splits\n",
    "    if accelerator.is_main_process:\n",
    "        train_episodes_dict = {}\n",
    "        val_episodes_dict = {}\n",
    "        # First, try to load validation episodes from data config\n",
    "        validation_episodes_loaded = False\n",
    "        \n",
    "        for i, cfg_item in enumerate(dataset_configs):\n",
    "            cfg_name = dataset_names[i]\n",
    "            \n",
    "            # Check if validation episodes are provided for this dataset\n",
    "            if use_validation_list and hasattr(cfg_item, 'validation_episodes') and cfg_item.validation_episodes:\n",
    "                validation_episodes_path = cfg_item.validation_episodes\n",
    "                if os.path.exists(validation_episodes_path):\n",
    "                    try:\n",
    "                        with open(validation_episodes_path, 'r') as f:\n",
    "                            validation_episodes_data = json.load(f)\n",
    "                        \n",
    "                        # Handle different JSON structures for this specific dataset\n",
    "                        if isinstance(validation_episodes_data, list):\n",
    "                            # Direct list format: [0, 1, 2, ...]\n",
    "                            val_episodes_dict[cfg_name] = validation_episodes_data\n",
    "                        elif isinstance(validation_episodes_data, dict):\n",
    "                            # Multi-dataset format: {dataset_name: [episodes]}\n",
    "                            if cfg_name in validation_episodes_data:\n",
    "                                val_episodes_dict[cfg_name] = validation_episodes_data[cfg_name]\n",
    "                            else:\n",
    "                                # Fallback to random splitting for this dataset\n",
    "                                val_episodes_dict[cfg_name] = None\n",
    "                        else:\n",
    "                            # Fallback to random splitting for this dataset\n",
    "                            val_episodes_dict[cfg_name] = None\n",
    "                        \n",
    "                        if val_episodes_dict[cfg_name] is not None:\n",
    "                            validation_episodes_loaded = True\n",
    "                            logging.info(f\"Loaded validation episodes for {cfg_name} from data config {validation_episodes_path}: {val_episodes_dict[cfg_name]}\")\n",
    "                        else:\n",
    "                            logging.warning(f\"Could not extract validation episodes for {cfg_name} from {validation_episodes_path}\")\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Failed to load validation episodes for {cfg_name} from {validation_episodes_path}: {e}\")\n",
    "                        val_episodes_dict[cfg_name] = None\n",
    "                else:\n",
    "                    logging.warning(f\"Validation episodes file {validation_episodes_path} not found for {cfg_name}\")\n",
    "                    val_episodes_dict[cfg_name] = None\n",
    "            else:\n",
    "                val_episodes_dict[cfg_name] = None\n",
    "        \n",
    "        # If no validation episodes were loaded, do random splitting\n",
    "        if not validation_episodes_loaded:\n",
    "            logging.warning(\"No validation episodes files found or use_validation_list is False, using random splitting\")\n",
    "            for i, cfg_item in enumerate(dataset_configs):\n",
    "                cfg_name = dataset_names[i]\n",
    "                \n",
    "                # Load metadata for this dataset\n",
    "                info_path = Path(cfg_item.root_dir) / \"meta\" / \"info.json\"\n",
    "                with open(info_path, \"r\") as f:\n",
    "                    total_episodes = json.load(f)[\"total_episodes\"]\n",
    "                \n",
    "                # Create random episode splits\n",
    "                all_episodes = list(range(total_episodes))\n",
    "                random.seed(seed + hash(cfg_name))\n",
    "                random.shuffle(all_episodes)\n",
    "                \n",
    "                n_val = max(1, int(total_episodes * val_split))\n",
    "                val_episodes = all_episodes[:n_val]\n",
    "                train_episodes = all_episodes[n_val:]\n",
    "                \n",
    "                val_episodes_dict[cfg_name] = val_episodes\n",
    "                train_episodes_dict[cfg_name] = train_episodes\n",
    "        else:\n",
    "            # Create train episodes as complement of validation episodes\n",
    "            for i, cfg_item in enumerate(dataset_configs):\n",
    "                cfg_name = dataset_names[i]\n",
    "                \n",
    "                # Load metadata for this dataset\n",
    "                info_path = Path(cfg_item.root_dir) / \"meta\" / \"info.json\"\n",
    "                with open(info_path, \"r\") as f:\n",
    "                    total_episodes = json.load(f)[\"total_episodes\"]\n",
    "                \n",
    "                if val_episodes_dict[cfg_name] is not None:\n",
    "                    # Create train episodes as complement of validation episodes\n",
    "                    all_episodes = set(range(total_episodes))\n",
    "                    val_episodes_set = set(val_episodes_dict[cfg_name])\n",
    "                    train_episodes = sorted(list(all_episodes - val_episodes_set))\n",
    "                    train_episodes_dict[cfg_name] = train_episodes\n",
    "                else:\n",
    "                    # Fallback to random splitting for this dataset\n",
    "                    all_episodes = list(range(total_episodes))\n",
    "                    random.seed(seed + hash(cfg_name))\n",
    "                    random.shuffle(all_episodes)\n",
    "                    \n",
    "                    n_val = max(1, int(total_episodes * val_split))\n",
    "                    val_episodes = all_episodes[:n_val]\n",
    "                    train_episodes = all_episodes[n_val:]\n",
    "                    \n",
    "                    val_episodes_dict[cfg_name] = val_episodes\n",
    "                    train_episodes_dict[cfg_name] = train_episodes\n",
    "    else:\n",
    "        # Other processes wait for broadcast\n",
    "        train_episodes_dict = None\n",
    "        val_episodes_dict = None\n",
    "    \n",
    "    # BROADCAST episode splits from main process to all processes\n",
    "    train_episodes_dict = broadcast_object_list([train_episodes_dict])[0]\n",
    "    val_episodes_dict = broadcast_object_list([val_episodes_dict])[0]\n",
    "    \n",
    "    # Check if we have mixture configs (data_cfg already created above)\n",
    "    is_mixture_cfg = hasattr(data_cfg, 'mixture_configs') and data_cfg.mixture_configs\n",
    "\n",
    "    # If a dataset provides an episodes allowlist file, intersect BEFORE sharding to avoid empty subsets\n",
    "    cfg_map = {}\n",
    "    if is_mixture_cfg:\n",
    "        for item in data_cfg.mixture_configs:\n",
    "            cfg_map[item.repo_id] = item\n",
    "    else:\n",
    "        cfg_map[\"main\"] = data_cfg\n",
    "\n",
    "    allowed_by_cfg = {}\n",
    "    for name, cfg_item in cfg_map.items():\n",
    "        allowlist_path = getattr(cfg_item, 'episodes_list_file', None)\n",
    "        allowed_set = None\n",
    "        try:\n",
    "            if allowlist_path and os.path.exists(allowlist_path):\n",
    "                with open(allowlist_path, 'r') as f:\n",
    "                    allowed_set = set(json.load(f))\n",
    "        except Exception:\n",
    "            allowed_set = None\n",
    "        allowed_by_cfg[name] = allowed_set\n",
    "\n",
    "    def _apply_allowlist(eps_dict: dict[str, list[int]]):\n",
    "        out = {}\n",
    "        for name, eps in eps_dict.items():\n",
    "            allowed = allowed_by_cfg.get(name)\n",
    "            if isinstance(eps, list) and allowed is not None:\n",
    "                filtered = [e for e in eps if e in allowed]\n",
    "                if len(filtered) == 0 and len(allowed) > 0:\n",
    "                    # Fallback to all allowed episodes to avoid empty datasets\n",
    "                    filtered = sorted(list(allowed))\n",
    "                elif len(allowed) == 0:\n",
    "                    raise Exception(f'The validation episodes are empty for the dataset: {name}. Please check the episodes list file: {allowlist_path} or the validation episodes list: meta/validation_episodes.json')\n",
    "                out[name] = filtered\n",
    "            else:\n",
    "                out[name] = eps\n",
    "        return out\n",
    "\n",
    "    train_episodes_dict = _apply_allowlist(train_episodes_dict)\n",
    "    val_episodes_dict = _apply_allowlist(val_episodes_dict)\n",
    "\n",
    "    # Partition validation episodes across processes so each worker evaluates on a different subset\n",
    "    # Keep keys identical across ranks to avoid collective mismatches during metric aggregation\n",
    "    if hasattr(data_cfg, 'mixture_configs') and data_cfg.mixture_configs:\n",
    "        world_size = accelerator.num_processes\n",
    "        rank = accelerator.process_index\n",
    "        val_episodes_dict_local = {}\n",
    "        for cfg_name, eps in val_episodes_dict.items():\n",
    "            if isinstance(eps, list):\n",
    "                if len(eps) >= world_size:\n",
    "                    # Round-robin sharding\n",
    "                    shard = eps[rank::world_size]\n",
    "                elif len(eps) > 0:\n",
    "                    # Too few episodes for strict sharding; assign at least one per rank via wrap-around\n",
    "                    shard = [eps[rank % len(eps)]]\n",
    "                else:\n",
    "                    shard = []\n",
    "                val_episodes_dict_local[cfg_name] = shard\n",
    "            else:\n",
    "                # Fallback: if episodes are not a list, keep as-is\n",
    "                val_episodes_dict_local[cfg_name] = eps\n",
    "        \n",
    "        \n",
    "        # Shard training episodes across processes with safe fallback for tiny splits\n",
    "        train_episodes_dict_local = {}\n",
    "        for cfg_name, eps in train_episodes_dict.items():\n",
    "            if isinstance(eps, list):\n",
    "                if len(eps) >= world_size:\n",
    "                    shard = eps[rank::world_size]\n",
    "                elif len(eps) > 0:\n",
    "                    shard = [eps[rank % len(eps)]]\n",
    "                else:\n",
    "                    shard = []\n",
    "                train_episodes_dict_local[cfg_name] = shard\n",
    "            else:\n",
    "                train_episodes_dict_local[cfg_name] = eps\n",
    "    else:\n",
    "        train_episodes_dict_local = train_episodes_dict\n",
    "        val_episodes_dict_local = val_episodes_dict\n",
    "\n",
    "\n",
    "    # ALL PROCESSES create datasets using their local episode splits\n",
    "    train_dataset, norm_stats = create_lerobot_dataset_by_config(\n",
    "        data_config_factory=data_config_factory,\n",
    "        model_config=model_config,\n",
    "        assets_dirs=assets_dirs,\n",
    "        episodes=train_episodes_dict_local,\n",
    "        normalization_mode=model_config.normalization_mode,\n",
    "        return_norm_stats=True,\n",
    "        map_to_unified_space=map_to_unified_space,\n",
    "        recompute_norm_stats=recompute_norm_stats,\n",
    "        **dataset_kwargs\n",
    "    )\n",
    "    # If mixture dataset, set per-rank RNG to reduce cross-rank duplication\n",
    "    if hasattr(train_dataset, \"set_rng\"):\n",
    "        try:\n",
    "            import numpy as _np\n",
    "            train_dataset.set_rng(_np.random.RandomState(seed + rank))\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    val_datasets_dict = {}\n",
    "    # data_cfg already created above, reuse it\n",
    "    is_mixture = hasattr(data_cfg, 'mixture_configs') and data_cfg.mixture_configs\n",
    "    \n",
    "    # Initialize output_pipeline_dict for all processes\n",
    "    output_pipeline_dict = {}\n",
    "    \n",
    "    for cfg_name, eps in val_episodes_dict_local.items():\n",
    "        if is_mixture:\n",
    "            cfg_item = next(item for item in data_cfg.mixture_configs if item.repo_id == cfg_name)\n",
    "        else:\n",
    "            cfg_item = data_cfg\n",
    "        norm_stats_item = norm_stats[cfg_item.repo_id]\n",
    "        factory = DummyFactory(cfg_item)\n",
    "        val_dataset = create_lerobot_dataset_by_config(\n",
    "            data_config_factory=factory,\n",
    "            model_config=model_config,\n",
    "            assets_dirs=assets_dirs,\n",
    "            episodes=eps,\n",
    "            normalization_mode=model_config.normalization_mode,\n",
    "            return_norm_stats=False,\n",
    "            recompute_norm_stats=False,\n",
    "            precomputed_norm_stats=norm_stats,\n",
    "            map_to_unified_space=map_to_unified_space,\n",
    "            **dataset_kwargs\n",
    "        )\n",
    "        val_datasets_dict[cfg_name] = val_dataset\n",
    "        output_pipeline_dict[cfg_name] = compose(\n",
    "            get_torch_output_transforms(\n",
    "                norm_stats=norm_stats_item,\n",
    "                policy_config=model_config,\n",
    "                data_config_factory=DummyFactory(cfg_item),\n",
    "                assets_dirs=assets_dirs,\n",
    "                normalization_mode=model_config.normalization_mode,\n",
    "                map_to_unified_space=map_to_unified_space\n",
    "        ))\n",
    "\n",
    "    return train_dataset, val_datasets_dict, norm_stats, output_pipeline_dict\n",
    "\n",
    "def instantiate_data_config(cfg: DictConfig, add_kwargs: dict = None):\n",
    "    \"\"\"Instantiate robotics dataset config.\n",
    "\n",
    "    - For mixture configs: mutate each nested data_config only for keys that already exist\n",
    "    (e.g., map_to_unified_space, map_to_humanoid), then instantiate with _recursive_=False.\n",
    "    - For individual configs: set keys on the config if they already exist, then instantiate with _recursive_=True.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        is_mixture = cfg._target_.split(\".\")[-1] == \"MixtureDataConfigFactory\"\n",
    "    except Exception:\n",
    "        is_mixture = False\n",
    "\n",
    "    if is_mixture and hasattr(cfg, \"datasets_with_weights\") and cfg.datasets_with_weights is not None:\n",
    "        assert cfg.data_configs is None, \"both datasets_with_weights and data_configs are set\"\n",
    "        assert cfg.weights is None, \"both datasets_with_weights and weights are set\"\n",
    "        datasets_list = [ds_cfg.path for ds_cfg in cfg.datasets_with_weights]\n",
    "        weights_list = [ds_cfg.weight for ds_cfg in cfg.datasets_with_weights]\n",
    "        cfg.data_configs = datasets_list\n",
    "        cfg.weights = weights_list\n",
    "        del cfg.datasets_with_weights\n",
    "\n",
    "    if add_kwargs:\n",
    "        if is_mixture and hasattr(cfg, \"data_configs\") and cfg.data_configs is not None:\n",
    "            # Update only existing keys in each nested dataset cfg to avoid unknown-arg errors\n",
    "            for idx in range(len(cfg.data_configs)):\n",
    "                dc = cfg.data_configs[idx]\n",
    "                try:\n",
    "                    # DictConfig supports 'in' and item assignment\n",
    "                    for k, v in add_kwargs.items():\n",
    "                        if isinstance(dc, DictConfig):\n",
    "                            if k in dc:\n",
    "                                dc[k] = v\n",
    "                        elif isinstance(dc, dict):\n",
    "                            if k in dc:\n",
    "                                dc[k] = v\n",
    "                except Exception:\n",
    "                    # Best-effort; skip problematic entries\n",
    "                    pass\n",
    "        else:\n",
    "            # Individual dataset config: only set keys that exist in cfg\n",
    "            for k, v in add_kwargs.items():\n",
    "                try:\n",
    "                    if k in cfg:\n",
    "                        cfg[k] = v\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    if is_mixture:\n",
    "        return hydra.utils.instantiate(cfg, _recursive_=False)\n",
    "    else:\n",
    "        return hydra.utils.instantiate(cfg, _recursive_=True)\n",
    "\n",
    "def get_datasets():\n",
    "    try:\n",
    "        OmegaConf.register_resolver(\n",
    "            \"_load_config\", lambda rel_path: OmegaConf.load(os.path.join(os.getcwd(), rel_path))\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    assets_dir = '/mnt/virtual_ai0001071-01239_SR006-nfs2/apanasevich/pi0_assets_v4'\n",
    "\n",
    "    cfg = torch.load('config.ckpt', weights_only=False)\n",
    "    cfg.robotics_dataset.data_configs = cfg.robotics_dataset.data_configs[:3]\n",
    "    cfg.robotics_dataset.weights = cfg.robotics_dataset.weights[:3]\n",
    "\n",
    "    map_to_unified_space = False\n",
    "    map_to_humanoid = False\n",
    "    add_kwargs = {\n",
    "        'map_to_unified_space': map_to_unified_space,\n",
    "        'map_to_humanoid': map_to_humanoid,\n",
    "    }\n",
    "    robotics_dataset_factory = instantiate_data_config(cfg.robotics_dataset, add_kwargs)\n",
    "    policy_config = hydra.utils.instantiate(cfg.policy.policy_config)\n",
    "\n",
    "    val_split = 0.02\n",
    "\n",
    "    robotics_dataset, val_datasets_dict, norm_stats, output_pipeline_dict = create_train_val_datasets_distributed(\n",
    "        data_config_factory=robotics_dataset_factory,\n",
    "        model_config=policy_config,\n",
    "        assets_dirs=assets_dir,\n",
    "        accelerator=accelerator,\n",
    "        val_split=val_split,\n",
    "        seed=42,\n",
    "        map_to_unified_space=map_to_unified_space,\n",
    "        use_validation_list=True,\n",
    "        recompute_norm_stats=False,\n",
    "    )\n",
    "    \n",
    "    return robotics_dataset, val_datasets_dict, norm_stats, output_pipeline_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "robotics_dataset, val_datasets_dict, norm_stats, output_pipeline_dict = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a36440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtl = DataLoader(robotics_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591c1c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dtl))['actions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c202171",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BSpline_Tokenizer(\n",
    "    num_basis=20, \n",
    "    vocab_size=1000,\n",
    "    degree_p=0,\n",
    "    gripper_zero_order=False,\n",
    "    num_dof=32, \n",
    "    seq_len=10, \n",
    "    gripper_indices=[],\n",
    "    init_pos=False,\n",
    "    device='cpu'\n",
    ")\n",
    "tokenizer.fit_parameters(dtl, max_samples=100)\n",
    "actions = next(iter(dtl))['actions']\n",
    "tokenizer.visualize_reconstruction_error(actions[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a7cade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6660e-08)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.compute_reconstruction_error(actions[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
