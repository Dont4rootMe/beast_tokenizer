{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6bc6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR006.nfs2/micromamba/envs/af_beast_calvin/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/constant_repos/lerobot-fork/lerobot/common/datasets/vlm_datasets/qwen_dataset_specific_augmentations.py:147: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
      "  A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),\n",
      "/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/constant_repos/lerobot-fork/lerobot/common/datasets/vlm_datasets/qwen_dataset_specific_augmentations.py:149: UserWarning: Argument(s) 'shift_limit' are not valid for transform OpticalDistortion\n",
      "  A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.2),\n",
      "/home/jovyan/shares/SR006.nfs2/micromamba/envs/af_beast_calvin/lib/python3.10/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/constant_repos/lerobot-fork/lerobot/common/datasets/vlm_datasets/qwen_dataset_specific_augmentations.py:215: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
      "/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/constant_repos/lerobot-fork/lerobot/common/datasets/vlm_datasets/qwen_dataset_specific_augmentations.py:287: UserWarning: Argument(s) 'alpha_affine' are not valid for transform ElasticTransform\n",
      "  augmentations.append(A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2))\n",
      "/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/constant_repos/lerobot-fork/lerobot/common/datasets/vlm_datasets/qwen_dataset_specific_augmentations.py:289: UserWarning: Argument(s) 'shift_limit' are not valid for transform OpticalDistortion\n",
      "  augmentations.append(A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.2))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"OPENPI_DATA_HOME\"] = (\n",
    "    \"/mnt/virtual_ai0001071-01239_SR006-nfs2/apanasevich/openpi/assets\"\n",
    ")\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/virtual_ai0001071-01239_SR006-nfs2/.cache/huggingface\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/mnt/virtual_ai0001071-01239_SR006-nfs2/.cache\"\n",
    "\n",
    "\n",
    "from lerobot.common.datasets.create_dataloader import create_lerobot_dataloader\n",
    "from lerobot.common.datasets.data_config import (\n",
    "    LeRobotAgibotTwoFingerDataConfig,\n",
    "    LeRobotAgibotDexHandDataConfig,\n",
    ")\n",
    "from rich_argparse import RichHelpFormatter\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "from accelerate.utils import InitProcessGroupKwargs\n",
    "from datetime import timedelta\n",
    "\n",
    "from lerobot.common.datasets.data_config import (\n",
    "    AssetsConfig as LeRobotAssetsConfig\n",
    ")\n",
    "from lerobot.common.datasets.data_config import DataConfig as LeRobotBaseDataConfig\n",
    "from lerobot.common.policies.pi0.configuration_pi0 import PI0Config\n",
    "from lerobot.common.policies.pi0fast.configuration_pi0fast import PI0FASTConfig\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from lerobot.common.utils.normalize import RunningStats, save as save_stats\n",
    "import torch.distributed as dist\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "from lerobot.common.datasets.create_dataloader import create_lerobot_dataset_by_config\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_data(\n",
    "    dataset_config_path: str, \n",
    "    assets_dir: str, \n",
    "    action_horizon: int | None = None,\n",
    "    action_dim: int | None = None\n",
    "):\n",
    "    cfg = OmegaConf.load(dataset_config_path)\n",
    "    cfg = OmegaConf.to_container(cfg, resolve=True)\n",
    "\n",
    "    if action_horizon is not None:\n",
    "        cfg['action_horizon'] = action_horizon\n",
    "    \n",
    "    data_config = instantiate(cfg)\n",
    "    model_cfg = PI0Config()\n",
    "    lerobot_dataset = create_lerobot_dataset_by_config(\n",
    "        data_config_factory=data_config,\n",
    "        model_config=model_cfg,\n",
    "        assets_dirs=assets_dir,\n",
    "        normalization_mode=\"mean_std\", #it does not matter\n",
    "        skip_norm_stats=True,\n",
    "        skip_model_transforms=True,\n",
    "        return_norm_stats=False,\n",
    "        )\n",
    "    \n",
    "    class InnerDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, dataset, action_horizon, action_dim):\n",
    "            self.dataset = dataset\n",
    "            self.action_horizon = action_horizon\n",
    "            self.action_dim = action_dim\n",
    "            \n",
    "            if self.action_dim is None:\n",
    "                self.action_dim = self.dataset[0]['actions'].shape[1]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.dataset[idx]['actions'][:self.action_horizon, :self.action_dim]\n",
    "\n",
    "    return InnerDataset(lerobot_dataset, action_horizon, action_dim), cfg['repo_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3c22e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Device 'None' is not available. Switching to 'cuda'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating LeRobot dataset for repo_id: aloha_aij\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating LeRobot dataset for repo_id: aloha_aij\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized PromptFromLeRobotTaskTorch with <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1858</span> tasks.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized PromptFromLeRobotTaskTorch with \u001b[1;36m1858\u001b[0m tasks.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">map_to_unified_space: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "map_to_unified_space: \u001b[3;91mFalse\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_config = \"/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/constant_repos/lerobot-fork/lerobot/conf/robotics_dataset/individual/aloha_aij.yaml\"\n",
    "assets_dir = \"/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/assets\"\n",
    "\n",
    "dataset, config_name = get_data(data_config, assets_dir, action_horizon=16, action_dim=14)\n",
    "\n",
    "# with open(f'{assets_dir}/{config_name}/norm_stats.json') as f:\n",
    "with open(f'{assets_dir}/aloha_aij_delta/norm_stats.json') as f:\n",
    "    norm_stats = json.load(f)\n",
    "max_stats = norm_stats['norm_stats']['actions']['q99']\n",
    "min_stats = norm_stats['norm_stats']['actions']['q01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de05d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/beast_tokenizer/beast/bspline_tokenizer.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float32):\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.84 GiB. GPU 0 has a total capacity of 79.21 GiB of which 5.28 GiB is free. Process 1675116 has 73.04 GiB memory in use. Including non-PyTorch memory, this process has 860.00 MiB memory in use. Of the allocated memory 164.30 MiB is allocated by PyTorch, and 17.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m trajs \u001b[38;5;241m=\u001b[39m (trajs \u001b[38;5;241m-\u001b[39m action_min) \u001b[38;5;241m/\u001b[39m (action_max \u001b[38;5;241m-\u001b[39m action_min)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m### tokenize the trajectory\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_reconstruction_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/beast_tokenizer/beast/bspline_tokenizer.py:30\u001b[0m, in \u001b[0;36mautocast_float32.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/beast_tokenizer/beast/bspline_tokenizer.py:348\u001b[0m, in \u001b[0;36mBSpline_Tokenizer.visualize_reconstruction_error\u001b[0;34m(self, raw_traj, max_vis_samples)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(raw_traj\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    347\u001b[0m     raw_traj \u001b[38;5;241m=\u001b[39m raw_traj\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m tokens, params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_traj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstruct_traj(tokens)\n\u001b[1;32m    350\u001b[0m pos \u001b[38;5;241m=\u001b[39m pos\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/micromamba/envs/af_beast_calvin/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/beast_tokenizer/beast/bspline_tokenizer.py:30\u001b[0m, in \u001b[0;36mautocast_float32.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/beast_tokenizer/beast/bspline_tokenizer.py:193\u001b[0m, in \u001b[0;36mBSpline_Tokenizer.encode\u001b[0;34m(self, trajs, update_bounds)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Extract joint trajectories using joint_indices\u001b[39;00m\n\u001b[1;32m    192\u001b[0m joint_trajs \u001b[38;5;241m=\u001b[39m trajs[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_indices]\n\u001b[0;32m--> 193\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_mp_params_from_trajs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_trajs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgripper_mp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# Extract gripper trajectories using gripper_indices\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     gripper_trajs \u001b[38;5;241m=\u001b[39m trajs[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgripper_indices]\n",
      "File \u001b[0;32m/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/beast_tokenizer/MP_lite_PyTorch/mp_pytorch/mp/uni_bspline.py:560\u001b[0m, in \u001b[0;36mUniformBSpline.learn_mp_params_from_trajs\u001b[0;34m(self, times, trajs, reg, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     weights_goal_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_scale\n\u001b[0;32m--> 560\u001b[0m basis_multi_dofs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasis_gn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasis_multi_dofs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_dof\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m weights_goal_scale\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# Solve this: Aw = B -> w = A^{-1} B\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Einsum_shape: [*add_dim, num_dof * num_times, num_dof * num_basis]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m#               [*add_dim, num_dof * num_times, num_dof * num_basis]\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m#            -> [*add_dim, num_dof * num_basis, num_dof * num_basis]\u001b[39;00m\n\u001b[1;32m    565\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...ki,...kj->...ij\u001b[39m\u001b[38;5;124m'\u001b[39m, basis_multi_dofs,\n\u001b[1;32m    566\u001b[0m                  basis_multi_dofs)\n",
      "File \u001b[0;32m/mnt/virtual_ai0001071-01239_SR006-nfs2/afedorov/projects/beast_tokenizer/MP_lite_PyTorch/mp_pytorch/basis_gn/uni_bspline_basis.py:349\u001b[0m, in \u001b[0;36mUniBSplineBasis.basis_multi_dofs\u001b[0;34m(self, times, num_dof)\u001b[0m\n\u001b[1;32m    342\u001b[0m     basis_single_dof_ \u001b[38;5;241m=\u001b[39m basis_single_dof[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_cond_order:\n\u001b[1;32m    343\u001b[0m                                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_ctrlp\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_cond_order]\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# basis_single_dof_ = basis_single_dof[..., self.init_cond_order:\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m#                                           self.init_cond_order+self.num_basis]\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Multiple Dofs, shape:\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# [*add_dim, num_dof * num_times, num_dof * num_basis]\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m basis_multi_dofs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madd_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_dof\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnum_dof\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_basis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Assemble\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_dof):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.84 GiB. GPU 0 has a total capacity of 79.21 GiB of which 5.28 GiB is free. Process 1675116 has 73.04 GiB memory in use. Including non-PyTorch memory, this process has 860.00 MiB memory in use. Of the allocated memory 164.30 MiB is allocated by PyTorch, and 17.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from beast.bspline_tokenizer import BSpline_Tokenizer\n",
    "\n",
    "num_indexes = np.random.randint(0, len(dataset), size=10_000)\n",
    "\n",
    "traj = np.stack([dataset[i].numpy() for i in num_indexes])\n",
    "num_dof = traj.shape[-1]\n",
    "seq_len = traj.shape[-2]\n",
    "\n",
    "action_min = np.array(min_stats[:num_dof])\n",
    "action_max = np.array(max_stats[:num_dof])\n",
    "\n",
    "tokenizer = BSpline_Tokenizer(num_dof=num_dof, num_basis=50, seq_len=seq_len, gripper_zero_order=False, gripper_indices=[]).to('cuda')\n",
    "\n",
    "trajs = torch.from_numpy(traj).to('cuda')\n",
    "action_min = torch.from_numpy(action_min).to('cuda')\n",
    "action_max = torch.from_numpy(action_max).to('cuda')\n",
    "\n",
    "### normalize the trajectory before tokenization\n",
    "trajs = (trajs - action_min) / (action_max - action_min)\n",
    "### tokenize the trajectory\n",
    "tokenizer.visualize_reconstruction_error(trajs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcaca17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 16, 14]), torch.Size([16]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajs.shape, action_min.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebb2cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m100\u001b[39m), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(100), np.random.randn(100))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
